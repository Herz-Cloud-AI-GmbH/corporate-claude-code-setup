# ============================================================================
# Claude Code multi-provider sample.env
#
# Goal: avoid hybrid configs.
# - GCP uses native Vertex AI (no LiteLLM).
# - Ollama/Copilot use LiteLLM proxy (Claude talks to localhost:4000).
#
# This file is a TEMPLATE. The active file is `.devcontainer/.env`.
# ============================================================================

DEVCONTAINER=true
CLAUDE_CODE_VERSION=latest

# ----------------------------------------------------------------------------
# Provider selector (used by scripts/manage.py)
# Options: gcp | ollama | copilot
# ----------------------------------------------------------------------------
PROFILE=gcp

# ============================================================================
# GCP (native Vertex AI) - used when PROFILE=gcp
# ============================================================================
# Required:
GCP_PROJECT_ID=your-gcp-project-id
CLOUD_ML_REGION=global

# Optional but recommended (native Vertex model IDs, NOT aliases like "sonnet"):
# Examples:
# - claude-sonnet-4-5@20250929
# - claude-haiku-4-5@20250925
ANTHROPIC_MODEL=claude-sonnet-4-5@20250929
ANTHROPIC_SMALL_FAST_MODEL=claude-haiku-4-5@20250925

# Notes:
# - No LITELLM_* or ANTHROPIC_BASE_URL is needed/used for GCP native mode.
# - ADC auth is handled by `gcloud auth application-default login`.

# ============================================================================
# LiteLLM proxy (used when PROFILE=ollama or PROFILE=copilot)
# ============================================================================
# Required for proxy mode:
LITELLM_PORT=4000
# Generate: openssl rand -hex 32
LITELLM_MASTER_KEY=your-litellm-master-key

# Which LiteLLM config to use (scripts/manage.py will set this on setup):
# LITELLM_CONFIG=config.ollama.yaml
# LITELLM_CONFIG=config.copilot.yaml
LITELLM_CONFIG=config.ollama.yaml

# ============================================================================
# Ollama notes (PROFILE=ollama)
# ============================================================================
# - Ollama must run on your HOST machine (outside the devcontainer)
# - Start: ollama serve
# - Pull:  ollama pull llama3.1:70b llama3.1:8b qwen2.5:3b
# - The container reaches it via host.docker.internal:11434

# ============================================================================
# GitHub Copilot notes (PROFILE=copilot)
# ============================================================================
# - Ensure youâ€™re logged into Copilot in VS Code/Cursor
# - LiteLLM uses the Copilot session to access Claude models
